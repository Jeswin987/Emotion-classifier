{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdl4V8dX977945br3Z5EvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeswin987/Emotion-classifier/blob/master/Emotion_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jeswin987/Emotion-classifier.git\n",
        "%cd Emotion-classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHUD1KEWIFjw",
        "outputId": "8b194d4e-3f53-4b3f-ea46-b69a27bfa645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Emotion-classifier'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 12.86 KiB | 4.29 MiB/s, done.\n",
            "/content/Emotion-classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/emotions (1).csv ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9_RMZYCIXin",
        "outputId": "3c703467-9eaf-4136-dfff-509cc84fd1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `mv /content/emotions (1).csv ./'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcBsuRYVCx7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d61ae3b4-860a-41ed-b4c2-78226513ccd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n",
            "✅ NLTK data downloaded successfully!\n",
            "🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀\n",
            "TWEET EMOTION ANALYSIS PIPELINE\n",
            "🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀\n",
            "==================================================\n",
            "ALERTING SYSTEM SETUP\n",
            "==================================================\n",
            "✅ Alerting system configured!\n",
            "📋 Alert thresholds:\n",
            "   accuracy_min: 0.75\n",
            "   anger_max: 0.3\n",
            "   sadness_max: 0.4\n",
            "   fear_max: 0.25\n",
            "   negative_emotions_max: 0.6\n",
            "✅ Found dataset: emotions (1).csv\n",
            "==================================================\n",
            "LOADING AND EXPLORING DATASET\n",
            "==================================================\n",
            "✅ Successfully loaded with utf-8 encoding\n",
            "✅ Dataset loaded successfully!\n",
            "📊 Dataset shape: (416809, 2)\n",
            "📋 Columns: ['text', 'label']\n",
            "\n",
            "🔍 First 5 rows:\n",
            "                                                text  label\n",
            "0      i just feel really helpless and heavy hearted      4\n",
            "1  ive enjoyed being able to slouch about relax a...      0\n",
            "2  i gave up my internship with the dmrg and am f...      4\n",
            "3                         i dont know i feel so lost      0\n",
            "4  i am a kindergarten teacher and i am thoroughl...      4\n",
            "\n",
            "⚠️ Missing values before cleaning:\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n",
            "\n",
            "🧹 Data cleaning summary:\n",
            "   Original size: 416809\n",
            "   After cleaning: 393822\n",
            "   Removed: 22987 rows\n",
            "\n",
            "📈 Class distribution:\n",
            "   0 (sadness): 118511 (30.1%)\n",
            "   1 (joy): 135030 (34.3%)\n",
            "   2 (love): 29468 (7.5%)\n",
            "   3 (anger): 54777 (13.9%)\n",
            "   4 (fear): 43629 (11.1%)\n",
            "   5 (surprise): 12407 (3.2%)\n",
            "\n",
            "📝 Sample texts by emotion:\n",
            "   SADNESS: \"ive enjoyed being able to slouch about relax and unwind and frankly needed it af...\"\n",
            "   JOY: \"i fear that they won t ever feel that delicious excitement of christmas eve at l...\"\n",
            "   LOVE: \"i would think that whomever would be lucky enough to stay in this suite must fee...\"\n",
            "   ANGER: \"i feel like a jerk because the library students who all claim to love scrabble c...\"\n",
            "   FEAR: \"i just feel really helpless and heavy hearted...\"\n",
            "   SURPRISE: \"im forever taking some time out to have a lie down because i feel weird...\"\n",
            "==================================================\n",
            "FEATURE PREPARATION\n",
            "==================================================\n",
            "🔄 Preprocessing text data...\n",
            "\n",
            "📝 Preprocessing examples:\n",
            "   Original: i just feel really helpless and heavy hearted...\n",
            "   Cleaned:  feel realli helpless heavi heart...\n",
            "\n",
            "   Original: ive enjoyed being able to slouch about relax and unwind and ...\n",
            "   Cleaned:  ive enjoy abl slouch relax unwind frankli need last week aro...\n",
            "\n",
            "   Original: i gave up my internship with the dmrg and am feeling distrau...\n",
            "   Cleaned:  gave internship dmrg feel distraught...\n",
            "\n",
            "📊 Splitting data...\n",
            "🔢 Creating TF-IDF features...\n",
            "⚡ Using subset of 10000 samples for faster training...\n",
            "✅ Feature preparation complete!\n",
            "   Training features shape: (10000, 10000)\n",
            "   Testing features shape: (78763, 10000)\n",
            "   Training samples: 10000\n",
            "   Testing samples: 78763\n",
            "==================================================\n",
            "SVM KERNEL COMPARISON\n",
            "==================================================\n",
            "\n",
            "🚀 Training SVM with LINEAR kernel...\n",
            "✅ LINEAR Kernel Results:\n",
            "   Accuracy: 0.8852\n",
            "   Precision: 0.8855\n",
            "   Recall: 0.8852\n",
            "   F1-Score: 0.8837\n",
            "\n",
            "📊 Detailed Classification Report (LINEAR):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.90      0.93      0.91     23702\n",
            "         joy       0.87      0.93      0.90     27006\n",
            "        love       0.84      0.68      0.75      5893\n",
            "       anger       0.93      0.85      0.89     10955\n",
            "        fear       0.91      0.83      0.86      8726\n",
            "    surprise       0.79      0.74      0.77      2481\n",
            "\n",
            "    accuracy                           0.89     78763\n",
            "   macro avg       0.87      0.83      0.85     78763\n",
            "weighted avg       0.89      0.89      0.88     78763\n",
            "\n",
            "📈 Confusion matrix saved as 'confusion_matrix_linear.png'\n",
            "\n",
            "🚀 Training SVM with POLY kernel...\n",
            "✅ POLY Kernel Results:\n",
            "   Accuracy: 0.6202\n",
            "   Precision: 0.7690\n",
            "   Recall: 0.6202\n",
            "   F1-Score: 0.5733\n",
            "\n",
            "📊 Detailed Classification Report (POLY):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.86      0.72      0.78     23702\n",
            "         joy       0.50      0.98      0.66     27006\n",
            "        love       0.95      0.12      0.21      5893\n",
            "       anger       0.97      0.25      0.39     10955\n",
            "        fear       0.94      0.20      0.33      8726\n",
            "    surprise       0.92      0.09      0.16      2481\n",
            "\n",
            "    accuracy                           0.62     78763\n",
            "   macro avg       0.86      0.39      0.42     78763\n",
            "weighted avg       0.77      0.62      0.57     78763\n",
            "\n",
            "📈 Confusion matrix saved as 'confusion_matrix_poly.png'\n",
            "\n",
            "🚀 Training SVM with RBF kernel...\n",
            "✅ RBF Kernel Results:\n",
            "   Accuracy: 0.8506\n",
            "   Precision: 0.8611\n",
            "   Recall: 0.8506\n",
            "   F1-Score: 0.8443\n",
            "\n",
            "📊 Detailed Classification Report (RBF):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.87      0.94      0.90     23702\n",
            "         joy       0.79      0.96      0.87     27006\n",
            "        love       0.89      0.53      0.66      5893\n",
            "       anger       0.95      0.77      0.85     10955\n",
            "        fear       0.93      0.72      0.81      8726\n",
            "    surprise       0.87      0.47      0.62      2481\n",
            "\n",
            "    accuracy                           0.85     78763\n",
            "   macro avg       0.88      0.73      0.78     78763\n",
            "weighted avg       0.86      0.85      0.84     78763\n",
            "\n",
            "📈 Confusion matrix saved as 'confusion_matrix_rbf.png'\n",
            "\n",
            "🚀 Training SVM with SIGMOID kernel...\n",
            "✅ SIGMOID Kernel Results:\n",
            "   Accuracy: 0.8838\n",
            "   Precision: 0.8842\n",
            "   Recall: 0.8838\n",
            "   F1-Score: 0.8822\n",
            "\n",
            "📊 Detailed Classification Report (SIGMOID):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.89      0.93      0.91     23702\n",
            "         joy       0.87      0.94      0.90     27006\n",
            "        love       0.85      0.67      0.75      5893\n",
            "       anger       0.93      0.84      0.89     10955\n",
            "        fear       0.90      0.83      0.86      8726\n",
            "    surprise       0.78      0.75      0.77      2481\n",
            "\n",
            "    accuracy                           0.88     78763\n",
            "   macro avg       0.87      0.83      0.85     78763\n",
            "weighted avg       0.88      0.88      0.88     78763\n",
            "\n",
            "📈 Confusion matrix saved as 'confusion_matrix_sigmoid.png'\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE COMPARISON SUMMARY\n",
            "==================================================\n",
            "         Accuracy  Precision  Recall  F1-Score\n",
            "linear     0.8852     0.8855  0.8852    0.8837\n",
            "poly       0.6202     0.7690  0.6202    0.5733\n",
            "rbf        0.8506     0.8611  0.8506    0.8443\n",
            "sigmoid    0.8838     0.8842  0.8838    0.8822\n",
            "\n",
            "📈 Accuracy comparison plot saved as 'kernel_accuracy_comparison.png'\n",
            "\n",
            "🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆\n",
            "BEST MODEL IDENTIFICATION\n",
            "🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆\n",
            "🥇 Best performing kernel: LINEAR\n",
            "🎯 Best accuracy: 0.8852\n",
            "📊 Full metrics for best model:\n",
            "   Accuracy: 0.8852\n",
            "   Precision: 0.8855\n",
            "   Recall: 0.8852\n",
            "   F1-Score: 0.8837\n",
            "\n",
            "🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️\n",
            "TRAINING FINAL SVM MODEL\n",
            "🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️🏗️\n",
            "✅ Final SVM model trained with LINEAR kernel\n",
            "============================================================\n",
            "CLUSTERING ANALYSIS\n",
            "============================================================\n",
            "🔍 Performing K-Means clustering...\n",
            "📊 Finding optimal number of clusters...\n",
            "📈 Clustering evaluation plots saved as 'clustering_evaluation.png'\n",
            "🎯 Using 6 clusters for detailed analysis...\n",
            "✅ Silhouette Score: 0.0056\n",
            "\n",
            "📋 Cluster composition analysis:\n",
            "emotion  anger  fear   joy  love  sadness  surprise\n",
            "cluster                                            \n",
            "0          999   827  2514   554     2253       227\n",
            "1           73    47   147    30      128        12\n",
            "2           28    45    63    20       74         8\n",
            "3          162    67   398   124      341        27\n",
            "4           13     6    41    10       36         3\n",
            "5           94    91   272    54      196        16\n",
            "📈 Cluster-emotion heatmap saved as 'cluster_emotion_heatmap.png'\n",
            "\n",
            "🎨 Creating cluster visualizations...\n",
            "   Computing t-SNE (this may take a moment)...\n",
            "📈 Dimensionality reduction visualizations saved as 'dimensionality_reduction_visualizations.png'\n",
            "\n",
            "🔍 Detailed cluster characteristics:\n",
            "\n",
            "   📌 Cluster 0:\n",
            "      Size: 7374 tweets\n",
            "      Dominant emotion: joy\n",
            "      Emotion distribution: {'joy': np.int64(2514), 'sadness': np.int64(2253), 'anger': np.int64(999), 'fear': np.int64(827), 'love': np.int64(554), 'surprise': np.int64(227)}\n",
            "      Sample 1: \"i have been feeling so defeated lately in motherhood that i cant believe god giv...\"\n",
            "      Sample 2: \"i feel as though i am on him about something he isn t doing or something he s do...\"\n",
            "\n",
            "   📌 Cluster 1:\n",
            "      Size: 437 tweets\n",
            "      Dominant emotion: joy\n",
            "      Emotion distribution: {'joy': np.int64(147), 'sadness': np.int64(128), 'anger': np.int64(73), 'fear': np.int64(47), 'love': np.int64(30), 'surprise': np.int64(12)}\n",
            "      Sample 1: \"i really feel that i should be more intelligent than i am...\"\n",
            "      Sample 2: \"im feeling really out of place and irritated...\"\n",
            "\n",
            "   📌 Cluster 2:\n",
            "      Size: 238 tweets\n",
            "      Dominant emotion: sadness\n",
            "      Emotion distribution: {'sadness': np.int64(74), 'joy': np.int64(63), 'fear': np.int64(45), 'anger': np.int64(28), 'love': np.int64(20), 'surprise': np.int64(8)}\n",
            "      Sample 1: \"i feel myself starting to get irritated by someone i just breathe in and out thr...\"\n",
            "      Sample 2: \"im starting to feel extremely restless...\"\n",
            "\n",
            "   📌 Cluster 3:\n",
            "      Size: 1119 tweets\n",
            "      Dominant emotion: joy\n",
            "      Emotion distribution: {'joy': np.int64(398), 'sadness': np.int64(341), 'anger': np.int64(162), 'love': np.int64(124), 'fear': np.int64(67), 'surprise': np.int64(27)}\n",
            "      Sample 1: \"i do feel like i am being milked by my beloved apple peeps and this is the decid...\"\n",
            "      Sample 2: \"i get the feeling the filmmakers must have liked that one a lot...\"\n",
            "\n",
            "   📌 Cluster 4:\n",
            "      Size: 109 tweets\n",
            "      Dominant emotion: joy\n",
            "      Emotion distribution: {'joy': np.int64(41), 'sadness': np.int64(36), 'anger': np.int64(13), 'love': np.int64(10), 'fear': np.int64(6), 'surprise': np.int64(3)}\n",
            "      Sample 1: \"i feel dirty link http draftbloger...\"\n",
            "      Sample 2: \"i have a feeling it will become one of those special dresses that stay in your w...\"\n",
            "\n",
            "   📌 Cluster 5:\n",
            "      Size: 723 tweets\n",
            "      Dominant emotion: joy\n",
            "      Emotion distribution: {'joy': np.int64(272), 'sadness': np.int64(196), 'anger': np.int64(94), 'fear': np.int64(91), 'love': np.int64(54), 'surprise': np.int64(16)}\n",
            "      Sample 1: \"i going through my mind was pass by the accessory shop and treat myself to sever...\"\n",
            "      Sample 2: \"i am feeling a little sentimental at all the things i have to be thankful for...\"\n",
            "\n",
            "📚 Performing topic modeling...\n",
            "\n",
            "🏷️ Topics discovered:\n",
            "   Topic 0: feel, like, time, life, littl, want, think, peopl, thing, week\n",
            "   Topic 1: feel, love, pretti, day, bit, littl, today, make, ive, right\n",
            "   Topic 2: feel, know, peopl, like, say, love, need, life, way, left\n",
            "   Topic 3: feel, like, hate, start, emot, use, day, littl, ive, person\n",
            "   Topic 4: feel, think, like, http, href, quit, make, irrit, post, bad\n",
            "   Topic 5: feel, realli, make, thing, want, sure, know, tri, friend, didnt\n",
            "\n",
            "✅ Clustering analysis complete!\n",
            "==================================================\n",
            "EMOTION PATTERN MONITORING\n",
            "==================================================\n",
            "📊 Current emotion distribution:\n",
            "   joy: 34.35%\n",
            "   sadness: 30.28%\n",
            "   anger: 13.69%\n",
            "   fear: 10.83%\n",
            "   love: 7.92%\n",
            "   surprise: 2.93%\n",
            "\n",
            "==================================================\n",
            "MODEL SAVING AND DOWNLOAD\n",
            "==================================================\n",
            "✅ Models saved successfully!\n",
            "   📁 svm_emotion_model_linear.pkl\n",
            "   📁 tfidf_vectorizer.pkl\n",
            "   📁 kmeans_model.pkl\n",
            "   📁 label_mapping.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bc7c510d-86c8-4dd7-b887-e0ffd494ec27\", \"svm_emotion_model_linear.pkl\", 1481475)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e304ddc7-9602-4593-a1d9-735718bb5bd6\", \"tfidf_vectorizer.pkl\", 382196)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8074d387-42f3-413a-8a59-256e27dc4f62\", \"kmeans_model.pkl\", 520727)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5088323e-9ac6-444b-b07c-c283f1247011\", \"label_mapping.pkl\", 77)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Files downloaded successfully!\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "INTERACTIVE EMOTION PREDICTION\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "🔍 Testing with sample texts:\n",
            "   Text: \"I'm so happy today!\"\n",
            "   Predicted Emotion: JOY\n",
            "\n",
            "   Text: \"This makes me really angry\"\n",
            "   Predicted Emotion: ANGER\n",
            "\n",
            "   Text: \"I'm scared of what might happen\"\n",
            "   Predicted Emotion: FEAR\n",
            "\n",
            "   Text: \"I love spending time with my family\"\n",
            "   Predicted Emotion: JOY\n",
            "\n",
            "   Text: \"I feel so sad and lonely\"\n",
            "   Predicted Emotion: SADNESS\n",
            "\n",
            "   Text: \"What a surprise that was!\"\n",
            "   Predicted Emotion: SURPRISE\n",
            "\n",
            "🎮 Try your own text (press Enter to skip):\n",
            "Enter text to predict emotion: Graduating feels amazing, but I'm going to miss all my college friends so much\n",
            "\n",
            "🎯 Predicted Emotion for \"Graduating feels amazing, but I'm going to miss all my college friends so much\": SURPRISE\n",
            "\n",
            "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
            "EMOTION ANALYSIS PIPELINE COMPLETE!\n",
            "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import joblib\n",
        "import warnings\n",
        "import smtplib\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "# NLTK imports for text preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Scikit-learn imports for machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "print(\"✅ NLTK data downloaded successfully!\")\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Emotion label mapping for better interpretation\n",
        "label_map = {\n",
        "    0: 'sadness',\n",
        "    1: 'joy',\n",
        "    2: 'love',\n",
        "    3: 'anger',\n",
        "    4: 'fear',\n",
        "    5: 'surprise'\n",
        "}\n",
        "\n",
        "# Global configuration for alerting\n",
        "ALERT_CONFIG = {\n",
        "    'email_enabled': False,\n",
        "    'discord_enabled': False,\n",
        "    'threshold_accuracy': 0.8,\n",
        "    'emotion_thresholds': {\n",
        "        'anger': 0.3,    # Alert if >30% anger\n",
        "        'sadness': 0.4,  # Alert if >40% sadness\n",
        "        'fear': 0.25     # Alert if >25% fear\n",
        "    }\n",
        "}\n",
        "\n",
        "def upload_dataset():\n",
        "    \"\"\"\n",
        "    Handle dataset upload with multiple fallback options\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the dataset file\n",
        "    \"\"\"\n",
        "    possible_files = ['emotions.csv', 'emotions (1).csv', '/content/emotions.csv', '/content/emotions (1).csv']\n",
        "\n",
        "    for filepath in possible_files:\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"✅ Found dataset: {filepath}\")\n",
        "            return filepath\n",
        "\n",
        "    print(\"📁 Dataset not found. Please upload your emotions.csv file.\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "\n",
        "        if filename != 'emotions.csv':\n",
        "            os.rename(filename, 'emotions.csv')\n",
        "            print(f\"✅ Renamed {filename} to emotions.csv\")\n",
        "\n",
        "        return 'emotions.csv'\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_and_explore_data(filepath):\n",
        "    \"\"\"\n",
        "    Load dataset and perform comprehensive exploration\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded and cleaned dataset\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"LOADING AND EXPLORING DATASET\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "        df = None\n",
        "        for encoding in encodings:\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, encoding=encoding)\n",
        "                print(f\"✅ Successfully loaded with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "        if df is None:\n",
        "            raise Exception(\"Could not decode file with any encoding\")\n",
        "\n",
        "        print(f\"✅ Dataset loaded successfully!\")\n",
        "        print(f\"📊 Dataset shape: {df.shape}\")\n",
        "        print(f\"📋 Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        required_columns = ['text', 'label']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            raise ValueError(f\"Missing required columns. Found: {df.columns.tolist()}\")\n",
        "\n",
        "        print(\"\\n🔍 First 5 rows:\")\n",
        "        print(df.head())\n",
        "\n",
        "        print(f\"\\n⚠️ Missing values before cleaning:\")\n",
        "        missing_before = df.isnull().sum()\n",
        "        print(missing_before)\n",
        "\n",
        "        original_size = len(df)\n",
        "        df.dropna(inplace=True)\n",
        "        df.drop_duplicates(subset='text', inplace=True)\n",
        "\n",
        "        print(f\"\\n🧹 Data cleaning summary:\")\n",
        "        print(f\"   Original size: {original_size}\")\n",
        "        print(f\"   After cleaning: {len(df)}\")\n",
        "        print(f\"   Removed: {original_size - len(df)} rows\")\n",
        "\n",
        "        valid_labels = set(range(6))\n",
        "        invalid_labels = set(df['label']) - valid_labels\n",
        "        if invalid_labels:\n",
        "            raise ValueError(f\"Invalid labels found: {invalid_labels}\")\n",
        "\n",
        "        df['emotion'] = df['label'].map(label_map)\n",
        "\n",
        "        print(f\"\\n📈 Class distribution:\")\n",
        "        class_dist = df['label'].value_counts().sort_index()\n",
        "        for label, count in class_dist.items():\n",
        "            emotion = label_map[label]\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"   {label} ({emotion}): {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample texts by emotion:\")\n",
        "        for label in sorted(df['label'].unique()):\n",
        "            emotion = label_map[label]\n",
        "            sample_text = df[df['label'] == label]['text'].iloc[0]\n",
        "            print(f\"   {emotion.upper()}: \\\"{sample_text[:80]}...\\\"\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Optimized text preprocessing function using stemming for speed\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text to preprocess\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned and preprocessed text\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def prepare_features(df, test_size=0.2, max_features=10000, subset_size=None):\n",
        "    \"\"\"\n",
        "    Prepare features using optimized TF-IDF vectorization\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataset with text and labels\n",
        "        test_size (float): Proportion of data for testing\n",
        "        max_features (int): Maximum number of TF-IDF features\n",
        "        subset_size (int): Optional subset size for faster training\n",
        "\n",
        "    Returns:\n",
        "        tuple: Training and testing sets with TF-IDF features, indices, and subset DataFrame\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"FEATURE PREPARATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"🔄 Preprocessing text data...\")\n",
        "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "    df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
        "    if len(df) == 0:\n",
        "        raise ValueError(\"No valid texts after preprocessing\")\n",
        "\n",
        "    print(\"\\n📝 Preprocessing examples:\")\n",
        "    for i in range(min(3, len(df))):\n",
        "        print(f\"   Original: {df['text'].iloc[i][:60]}...\")\n",
        "        print(f\"   Cleaned:  {df['cleaned_text'].iloc[i][:60]}...\")\n",
        "        print()\n",
        "\n",
        "    X = df['cleaned_text']\n",
        "    y = df['label']\n",
        "\n",
        "    print(\"📊 Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"🔢 Creating TF-IDF features...\")\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        min_df=5,\n",
        "        max_df=0.8,\n",
        "        ngram_range=(1, 2),\n",
        "        stop_words='english',\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    train_indices = X_train.index\n",
        "    if subset_size and subset_size < X_train_tfidf.shape[0]:\n",
        "        print(f\"⚡ Using subset of {subset_size} samples for faster training...\")\n",
        "        subset_indices = np.random.choice(X_train_tfidf.shape[0], subset_size, replace=False)\n",
        "        X_train_tfidf = X_train_tfidf[subset_indices]\n",
        "        y_train = y_train.iloc[subset_indices]\n",
        "        train_indices = X_train.index[subset_indices]\n",
        "\n",
        "    print(f\"✅ Feature preparation complete!\")\n",
        "    print(f\"   Training features shape: {X_train_tfidf.shape}\")\n",
        "    print(f\"   Testing features shape: {X_test_tfidf.shape}\")\n",
        "    print(f\"   Training samples: {len(y_train)}\")\n",
        "    print(f\"   Testing samples: {len(y_test)}\")\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf, y_train, y_test, tfidf, df, train_indices\n",
        "\n",
        "def evaluate_svm_kernel(kernel_type, X_train, X_test, y_train, y_test, show_confusion_matrix=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate SVM with specific kernel\n",
        "\n",
        "    Args:\n",
        "        kernel_type (str): Type of SVM kernel\n",
        "        X_train, X_test: Training and testing features\n",
        "        y_train, y_test: Training and testing labels\n",
        "        show_confusion_matrix (bool): Whether to display confusion matrix\n",
        "\n",
        "    Returns:\n",
        "        dict: Model performance metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n🚀 Training SVM with {kernel_type.upper()} kernel...\")\n",
        "\n",
        "    model_params = {\n",
        "        'kernel': kernel_type,\n",
        "        'random_state': 42,\n",
        "        'probability': True\n",
        "    }\n",
        "    if kernel_type == 'poly':\n",
        "        model_params['degree'] = 3\n",
        "    elif kernel_type == 'rbf':\n",
        "        model_params['gamma'] = 'scale'\n",
        "\n",
        "    model = SVC(**model_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    print(f\"✅ {kernel_type.upper()} Kernel Results:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {report_dict['weighted avg']['precision']:.4f}\")\n",
        "    print(f\"   Recall: {report_dict['weighted avg']['recall']:.4f}\")\n",
        "    print(f\"   F1-Score: {report_dict['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    print(f\"\\n📊 Detailed Classification Report ({kernel_type.upper()}):\")\n",
        "    emotion_names = [label_map[i] for i in sorted(label_map.keys())]\n",
        "    print(classification_report(y_test, y_pred, target_names=emotion_names))\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=emotion_names,\n",
        "                    yticklabels=emotion_names)\n",
        "        plt.title(f'Confusion Matrix - {kernel_type.upper()} Kernel', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Predicted Emotion', fontsize=12)\n",
        "        plt.ylabel('Actual Emotion', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'confusion_matrix_{kernel_type}.png')\n",
        "        plt.close()\n",
        "        print(f\"📈 Confusion matrix saved as 'confusion_matrix_{kernel_type}.png'\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report_dict['weighted avg']['precision'],\n",
        "        'recall': report_dict['weighted avg']['recall'],\n",
        "        'f1-score': report_dict['weighted avg']['f1-score'],\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "def perform_clustering_analysis(X_tfidf, df, train_indices, n_clusters=6):\n",
        "    \"\"\"\n",
        "    Perform comprehensive clustering analysis on emotion data\n",
        "\n",
        "    Args:\n",
        "        X_tfidf: TF-IDF feature matrix\n",
        "        df: Original dataframe with emotions\n",
        "        train_indices: Indices of the training subset\n",
        "        n_clusters: Number of clusters for K-means\n",
        "\n",
        "    Returns:\n",
        "        dict: Clustering results and analysis\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"CLUSTERING ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    clustering_results = {}\n",
        "\n",
        "    print(\"🔍 Performing K-Means clustering...\")\n",
        "    inertias = []\n",
        "    silhouette_scores = []\n",
        "    K_range = range(2, 11)\n",
        "\n",
        "    print(\"📊 Finding optimal number of clusters...\")\n",
        "    for k in K_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans.fit(X_tfidf)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "        if k > 1:\n",
        "            silhouette_scores.append(silhouette_score(X_tfidf, kmeans.labels_))\n",
        "        else:\n",
        "            silhouette_scores.append(0)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(K_range, inertias, 'bo-')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.title('Elbow Method for Optimal k')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(K_range, silhouette_scores, 'ro-')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title('Silhouette Analysis')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('clustering_evaluation.png')\n",
        "    plt.close()\n",
        "    print(\"📈 Clustering evaluation plots saved as 'clustering_evaluation.png'\")\n",
        "\n",
        "    print(f\"🎯 Using {n_clusters} clusters for detailed analysis...\")\n",
        "    kmeans_final = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans_final.fit_predict(X_tfidf)\n",
        "\n",
        "    df_clustered = df.loc[train_indices].copy()\n",
        "    df_clustered['cluster'] = cluster_labels\n",
        "\n",
        "    silhouette_avg = silhouette_score(X_tfidf, cluster_labels)\n",
        "    print(f\"✅ Silhouette Score: {silhouette_avg:.4f}\")\n",
        "\n",
        "    print(\"\\n📋 Cluster composition analysis:\")\n",
        "    cluster_emotion_dist = pd.crosstab(df_clustered['cluster'], df_clustered['emotion'])\n",
        "    print(cluster_emotion_dist)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(cluster_emotion_dist, annot=True, fmt='d', cmap='YlOrRd')\n",
        "    plt.title('Cluster vs Emotion Distribution Heatmap', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Emotion')\n",
        "    plt.ylabel('Cluster')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('cluster_emotion_heatmap.png')\n",
        "    plt.close()\n",
        "    print(\"📈 Cluster-emotion heatmap saved as 'cluster_emotion_heatmap.png'\")\n",
        "\n",
        "    print(\"\\n🎨 Creating cluster visualizations...\")\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_pca = pca.fit_transform(X_tfidf.toarray())\n",
        "\n",
        "    print(\"   Computing t-SNE (this may take a moment)...\")\n",
        "    subset_size = min(5000, X_tfidf.shape[0])\n",
        "    indices = np.random.choice(X_tfidf.shape[0], subset_size, replace=False)\n",
        "    X_tfidf_subset = X_tfidf[indices]\n",
        "    cluster_labels_subset = cluster_labels[indices]\n",
        "    emotion_labels_subset = df_clustered['label'].iloc[indices]\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "    X_tsne = tsne.fit_transform(X_tfidf_subset.toarray())\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter1 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
        "    plt.title('PCA - Clusters', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    plt.colorbar(scatter1)\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    scatter2 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_clustered['label'], cmap='tab10', alpha=0.6)\n",
        "    plt.title('PCA - Emotions', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    plt.colorbar(scatter2)\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    scatter3 = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels_subset, cmap='viridis', alpha=0.6)\n",
        "    plt.title('t-SNE - Clusters', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('t-SNE 1')\n",
        "    plt.ylabel('t-SNE 2')\n",
        "    plt.colorbar(scatter3)\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    scatter4 = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=emotion_labels_subset, cmap='tab10', alpha=0.6)\n",
        "    plt.title('t-SNE - Emotions', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('t-SNE 1')\n",
        "    plt.ylabel('t-SNE 2')\n",
        "    plt.colorbar(scatter4)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dimensionality_reduction_visualizations.png')\n",
        "    plt.close()\n",
        "    print(\"📈 Dimensionality reduction visualizations saved as 'dimensionality_reduction_visualizations.png'\")\n",
        "\n",
        "    print(\"\\n🔍 Detailed cluster characteristics:\")\n",
        "    for cluster_id in range(n_clusters):\n",
        "        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]\n",
        "        dominant_emotion = cluster_data['emotion'].mode().iloc[0] if not cluster_data.empty else \"None\"\n",
        "        emotion_dist = cluster_data['emotion'].value_counts()\n",
        "\n",
        "        print(f\"\\n   📌 Cluster {cluster_id}:\")\n",
        "        print(f\"      Size: {len(cluster_data)} tweets\")\n",
        "        print(f\"      Dominant emotion: {dominant_emotion}\")\n",
        "        print(f\"      Emotion distribution: {dict(emotion_dist)}\")\n",
        "\n",
        "        sample_tweets = cluster_data['text'].head(2).tolist()\n",
        "        for i, tweet in enumerate(sample_tweets):\n",
        "            print(f\"      Sample {i+1}: \\\"{tweet[:80]}...\\\"\")\n",
        "\n",
        "    print(\"\\n📚 Performing topic modeling...\")\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import CountVectorizer\n",
        "        vectorizer = CountVectorizer(max_features=1000, stop_words='english', min_df=5)\n",
        "        X_counts = vectorizer.fit_transform(df_clustered['cleaned_text'])\n",
        "        lda = LatentDirichletAllocation(n_components=n_clusters, random_state=42, max_iter=10)\n",
        "        lda.fit(X_counts)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        print(\"\\n🏷️ Topics discovered:\")\n",
        "        for topic_idx, topic in enumerate(lda.components_):\n",
        "            top_words = [feature_names[i] for i in topic.argsort()[-10:][::-1]]\n",
        "            print(f\"   Topic {topic_idx}: {', '.join(top_words)}\")\n",
        "\n",
        "        clustering_results['topics'] = {\n",
        "            'model': lda,\n",
        "            'vectorizer': vectorizer,\n",
        "            'feature_names': feature_names\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ Topic modeling failed: {e}\")\n",
        "\n",
        "    clustering_results.update({\n",
        "        'kmeans_model': kmeans_final,\n",
        "        'cluster_labels': cluster_labels,\n",
        "        'silhouette_score': silhouette_avg,\n",
        "        'cluster_emotion_dist': cluster_emotion_dist,\n",
        "        'pca_model': pca,\n",
        "        'tsne_model': tsne,\n",
        "        'clustered_data': df_clustered\n",
        "    })\n",
        "\n",
        "    print(\"\\n✅ Clustering analysis complete!\")\n",
        "    return clustering_results\n",
        "\n",
        "def setup_alerting_system():\n",
        "    \"\"\"\n",
        "    Configure alerting system for monitoring emotion patterns\n",
        "\n",
        "    Returns:\n",
        "        dict: Alerting configuration\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ALERTING SYSTEM SETUP\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    alert_config = {\n",
        "        'enabled': True,\n",
        "        'email_alerts': {\n",
        "            'enabled': False,\n",
        "            'smtp_server': 'smtp.gmail.com',\n",
        "            'smtp_port': 587,\n",
        "            'sender_email': 'your-email@gmail.com',\n",
        "            'sender_password': 'your-app-password',\n",
        "            'recipient_email': 'recipient@gmail.com'\n",
        "        },\n",
        "        'discord_alerts': {\n",
        "            'enabled': False,\n",
        "            'webhook_url': 'https://discord.com/api/webhooks/YOUR_WEBHOOK_URL'\n",
        "        },\n",
        "        'console_alerts': {\n",
        "            'enabled': True\n",
        "        },\n",
        "        'thresholds': {\n",
        "            'accuracy_min': 0.75,\n",
        "            'anger_max': 0.30,\n",
        "            'sadness_max': 0.40,\n",
        "            'fear_max': 0.25,\n",
        "            'negative_emotions_max': 0.60\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"✅ Alerting system configured!\")\n",
        "    print(\"📋 Alert thresholds:\")\n",
        "    for threshold, value in alert_config['thresholds'].items():\n",
        "        print(f\"   {threshold}: {value}\")\n",
        "\n",
        "    return alert_config\n",
        "\n",
        "def send_alert(alert_type, message, details, alert_config):\n",
        "    \"\"\"\n",
        "    Send alerts through configured channels\n",
        "\n",
        "    Args:\n",
        "        alert_type (str): Type of alert (accuracy, emotion_threshold, etc.)\n",
        "        message (str): Alert message\n",
        "        details (dict): Additional details\n",
        "        alert_config (dict): Alerting configuration\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    if alert_config['console_alerts']['enabled']:\n",
        "        print(\"\\n\" + \"🚨\" * 20)\n",
        "        print(f\"ALERT: {alert_type.upper()}\")\n",
        "        print(f\"Time: {timestamp}\")\n",
        "        print(f\"Message: {message}\")\n",
        "        if details:\n",
        "            print(\"Details:\")\n",
        "            for key, value in details.items():\n",
        "                print(f\"  {key}: {value}\")\n",
        "        print(\"🚨\" * 20)\n",
        "\n",
        "    if alert_config['email_alerts']['enabled']:\n",
        "        try:\n",
        "            send_email_alert(alert_type, message, details, alert_config['email_alerts'], timestamp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Email alert failed: {e}\")\n",
        "\n",
        "    if alert_config['discord_alerts']['enabled']:\n",
        "        try:\n",
        "            send_discord_alert(alert_type, message, details, alert_config['discord_alerts'], timestamp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Discord alert failed: {e}\")\n",
        "\n",
        "def send_email_alert(alert_type, message, details, email_config, timestamp):\n",
        "    \"\"\"Send email alert\"\"\"\n",
        "    subject = f\"Emotion Analysis Alert: {alert_type}\"\n",
        "    body = f\"\"\"\n",
        "    Emotion Analysis Alert\n",
        "\n",
        "    Type: {alert_type}\n",
        "    Time: {timestamp}\n",
        "    Message: {message}\n",
        "\n",
        "    Details:\n",
        "    \"\"\"\n",
        "    for key, value in details.items():\n",
        "        body += f\"  {key}: {value}\\n\"\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = email_config['sender_email']\n",
        "    msg['To'] = email_config['recipient_email']\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "    server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])\n",
        "    server.starttls()\n",
        "    server.login(email_config['sender_email'], email_config['sender_password'])\n",
        "    server.send_message(msg)\n",
        "    server.quit()\n",
        "\n",
        "def send_discord_alert(alert_type, message, details, discord_config, timestamp):\n",
        "    \"\"\"Send Discord webhook alert\"\"\"\n",
        "    webhook_data = {\n",
        "        \"embeds\": [{\n",
        "            \"title\": f\"🚨 Emotion Analysis Alert: {alert_type}\",\n",
        "            \"description\": message,\n",
        "            \"color\": 16711680,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"fields\": [{\"name\": key, \"value\": str(value), \"inline\": True} for key, value in details.items()]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    response = requests.post(discord_config['webhook_url'], json=webhook_data)\n",
        "    response.raise_for_status()\n",
        "\n",
        "def monitor_emotion_patterns(results, clustering_results, alert_config):\n",
        "    \"\"\"\n",
        "    Monitor emotion patterns and trigger alerts if thresholds are exceeded\n",
        "\n",
        "    Args:\n",
        "        results (dict): SVM classification results\n",
        "        clustering_results (dict): Clustering analysis results\n",
        "        alert_config (dict): Alerting configuration\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"EMOTION PATTERN MONITORING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    best_accuracy = max([results[kernel]['accuracy'] for kernel in results.keys()])\n",
        "\n",
        "    if best_accuracy < alert_config['thresholds']['accuracy_min']:\n",
        "        send_alert(\n",
        "            'low_accuracy',\n",
        "            f'Model accuracy ({best_accuracy:.4f}) is below threshold ({alert_config[\"thresholds\"][\"accuracy_min\"]})',\n",
        "            {'accuracy': best_accuracy, 'threshold': alert_config['thresholds']['accuracy_min']},\n",
        "            alert_config\n",
        "        )\n",
        "\n",
        "    df_clustered = clustering_results['clustered_data']\n",
        "    emotion_dist = df_clustered['emotion'].value_counts(normalize=True)\n",
        "\n",
        "    print(\"📊 Current emotion distribution:\")\n",
        "    for emotion, percentage in emotion_dist.items():\n",
        "        print(f\"   {emotion}: {percentage:.2%}\")\n",
        "\n",
        "    negative_emotions = ['anger', 'sadness', 'fear']\n",
        "    for emotion in negative_emotions:\n",
        "        if emotion in emotion_dist:\n",
        "            percentage = emotion_dist[emotion]\n",
        "            threshold_key = f'{emotion}_max'\n",
        "            if threshold_key in alert_config['thresholds']:\n",
        "                threshold = alert_config['thresholds'][threshold_key]\n",
        "                if percentage > threshold:\n",
        "                    send_alert(\n",
        "                        f'high_{emotion}',\n",
        "                        f'High {emotion} detected: {percentage:.2%} (threshold: {threshold:.2%})',\n",
        "                        {\n",
        "                            'emotion': emotion,\n",
        "                            'percentage': f'{percentage:.2%}',\n",
        "                            'threshold': f'{threshold:.2%}',\n",
        "                            'sample_count': int(percentage * len(df_clustered))\n",
        "                        },\n",
        "                        alert_config\n",
        "                    )\n",
        "\n",
        "    negative_percentage = sum([emotion_dist.get(emotion, 0) for emotion in negative_emotions])\n",
        "    if negative_percentage > alert_config['thresholds']['negative_emotions_max']:\n",
        "        send_alert(\n",
        "            'high_negative_emotions',\n",
        "            f'High negative emotions detected: {negative_percentage:.2%}',\n",
        "            {\n",
        "                'negative_percentage': f'{negative_percentage:.2%}',\n",
        "                'threshold': f'{alert_config[\"thresholds\"][\"negative_emotions_max\"]:.2%}',\n",
        "                'breakdown': {emotion: f'{emotion_dist.get(emotion, 0):.2%}' for emotion in negative_emotions}\n",
        "            },\n",
        "            alert_config\n",
        "        )\n",
        "\n",
        "def compare_kernels(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Compare performance of different SVM kernels\n",
        "\n",
        "    Args:\n",
        "        X_train, X_test: Training and testing features\n",
        "        y_train, y_test: Training and testing labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: Results dictionary and best kernel name\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"SVM KERNEL COMPARISON\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    results = {}\n",
        "\n",
        "    for kernel in kernels:\n",
        "        results[kernel] = evaluate_svm_kernel(kernel, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    comparison_data = {\n",
        "        kernel: {\n",
        "            'Accuracy': results[kernel]['accuracy'],\n",
        "            'Precision': results[kernel]['precision'],\n",
        "            'Recall': results[kernel]['recall'],\n",
        "            'F1-Score': results[kernel]['f1-score']\n",
        "        }\n",
        "        for kernel in kernels\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame(comparison_data).T\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(kernels, [results[k]['accuracy'] for k in kernels],\n",
        "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "    plt.title('SVM Kernel Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Kernel Type', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    for bar, accuracy in zip(bars, [results[k]['accuracy'] for k in kernels]):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{accuracy:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('kernel_accuracy_comparison.png')\n",
        "    plt.close()\n",
        "    print(\"\\n📈 Accuracy comparison plot saved as 'kernel_accuracy_comparison.png'\")\n",
        "\n",
        "    best_kernel = results_df['Accuracy'].idxmax()\n",
        "    best_accuracy = results_df.loc[best_kernel, 'Accuracy']\n",
        "\n",
        "    print(\"\\n\" + \"🏆\" * 50)\n",
        "    print(\"BEST MODEL IDENTIFICATION\")\n",
        "    print(\"🏆\" * 50)\n",
        "    print(f\"🥇 Best performing kernel: {best_kernel.upper()}\")\n",
        "    print(f\"🎯 Best accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"📊 Full metrics for best model:\")\n",
        "    for metric, value in results_df.loc[best_kernel].items():\n",
        "        print(f\"   {metric}: {value:.4f}\")\n",
        "\n",
        "    return results, best_kernel\n",
        "\n",
        "def save_and_download_models(best_model, tfidf_vectorizer, kmeans_model, best_kernel):\n",
        "    \"\"\"\n",
        "    Save trained models and download them\n",
        "\n",
        "    Args:\n",
        "        best_model: Trained SVM model\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        kmeans_model: Trained K-means model\n",
        "        best_kernel (str): Name of best performing kernel\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"MODEL SAVING AND DOWNLOAD\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        model_filename = f'svm_emotion_model_{best_kernel}.pkl'\n",
        "        joblib.dump(best_model, model_filename)\n",
        "        joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "        joblib.dump(kmeans_model, 'kmeans_model.pkl')\n",
        "        joblib.dump(label_map, 'label_mapping.pkl')\n",
        "\n",
        "        print(\"✅ Models saved successfully!\")\n",
        "        print(f\"   📁 {model_filename}\")\n",
        "        print(f\"   📁 tfidf_vectorizer.pkl\")\n",
        "        print(f\"   📁 kmeans_model.pkl\")\n",
        "        print(f\"   📁 label_mapping.pkl\")\n",
        "\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(model_filename)\n",
        "            files.download('tfidf_vectorizer.pkl')\n",
        "            files.download('kmeans_model.pkl')\n",
        "            files.download('label_mapping.pkl')\n",
        "            print(\"📥 Files downloaded successfully!\")\n",
        "        except:\n",
        "            print(\"⚠️ Download not available (not in Colab environment)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving models: {e}\")\n",
        "\n",
        "def create_prediction_function(model, vectorizer):\n",
        "    \"\"\"\n",
        "    Create a function for predicting emotions from new text\n",
        "\n",
        "    Args:\n",
        "        model: Trained SVM model\n",
        "        vectorizer: Fitted TF-IDF vectorizer\n",
        "\n",
        "    Returns:\n",
        "        function: Prediction function\n",
        "    \"\"\"\n",
        "    def predict_emotion(text):\n",
        "        \"\"\"\n",
        "        Predict emotion for given text\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            str: Predicted emotion\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return \"Invalid input\"\n",
        "\n",
        "        cleaned_text = preprocess_text(text)\n",
        "        if not cleaned_text:\n",
        "            return \"Empty after preprocessing\"\n",
        "\n",
        "        text_vector = vectorizer.transform([cleaned_text])\n",
        "        pred_label = model.predict(text_vector)[0]\n",
        "        emotion = label_map.get(pred_label, \"Unknown\")\n",
        "\n",
        "        return emotion\n",
        "\n",
        "    return predict_emotion\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function executing the complete emotion analysis pipeline\n",
        "    \"\"\"\n",
        "    print(\"🚀\" * 20)\n",
        "    print(\"TWEET EMOTION ANALYSIS PIPELINE\")\n",
        "    print(\"🚀\" * 20)\n",
        "\n",
        "    try:\n",
        "        alert_config = setup_alerting_system()\n",
        "\n",
        "        filepath = upload_dataset()\n",
        "        if not filepath:\n",
        "            print(\"❌ Cannot proceed without dataset\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = load_and_explore_data(filepath)\n",
        "        if df is None:\n",
        "            print(\"❌ Failed to load dataset\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        X_train, X_test, y_train, y_test, tfidf, df, train_indices = prepare_features(\n",
        "            df, test_size=0.2, max_features=10000, subset_size=10000\n",
        "        )\n",
        "\n",
        "        results, best_kernel = compare_kernels(X_train, X_test, y_train, y_test)\n",
        "\n",
        "        print(\"\\n\" + \"🏗️\" * 50)\n",
        "        print(\"TRAINING FINAL SVM MODEL\")\n",
        "        print(\"🏗️\" * 50)\n",
        "\n",
        "        model_params = {\n",
        "            'kernel': best_kernel,\n",
        "            'probability': True,\n",
        "            'random_state': 42\n",
        "        }\n",
        "        if best_kernel == 'poly':\n",
        "            model_params['degree'] = 3\n",
        "        elif best_kernel == 'rbf':\n",
        "            model_params['gamma'] = 'scale'\n",
        "\n",
        "        final_model = SVC(**model_params)\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"✅ Final SVM model trained with {best_kernel.upper()} kernel\")\n",
        "\n",
        "        clustering_results = perform_clustering_analysis(X_train, df, train_indices, n_clusters=6)\n",
        "\n",
        "        monitor_emotion_patterns(results, clustering_results, alert_config)\n",
        "\n",
        "        save_and_download_models(final_model, tfidf, clustering_results['kmeans_model'], best_kernel)\n",
        "\n",
        "        predict_emotion = create_prediction_function(final_model, tfidf)\n",
        "\n",
        "        print(\"\\n\" + \"🎯\" * 50)\n",
        "        print(\"INTERACTIVE EMOTION PREDICTION\")\n",
        "        print(\"🎯\" * 50)\n",
        "\n",
        "        sample_texts = [\n",
        "            \"I'm so happy today!\",\n",
        "            \"This makes me really angry\",\n",
        "            \"I'm scared of what might happen\",\n",
        "            \"I love spending time with my family\",\n",
        "            \"I feel so sad and lonely\",\n",
        "            \"What a surprise that was!\"\n",
        "        ]\n",
        "\n",
        "        print(\"🔍 Testing with sample texts:\")\n",
        "        for text in sample_texts:\n",
        "            emotion = predict_emotion(text)\n",
        "            print(f\"   Text: \\\"{text}\\\"\")\n",
        "            print(f\"   Predicted Emotion: {emotion.upper()}\")\n",
        "            print()\n",
        "\n",
        "        print(\"🎮 Try your own text (press Enter to skip):\")\n",
        "        try:\n",
        "            user_input = input(\"Enter text to predict emotion: \").strip()\n",
        "            if user_input:\n",
        "                predicted_emotion = predict_emotion(user_input)\n",
        "                print(f\"\\n🎯 Predicted Emotion for \\\"{user_input}\\\": {predicted_emotion.upper()}\")\n",
        "        except:\n",
        "            print(\"⚠️ Interactive input not available in this environment\")\n",
        "\n",
        "        print(\"\\n\" + \"🎉\" * 50)\n",
        "        print(\"EMOTION ANALYSIS PIPELINE COMPLETE!\")\n",
        "        print(\"🎉\" * 50)\n",
        "\n",
        "        return results, clustering_results, tfidf, predict_emotion\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred: {str(e)}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, clustering_results, vectorizer, predictor = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vW8bPpF5klYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}